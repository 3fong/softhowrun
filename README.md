## 程序是怎么跑起来的

CPU -> 二进制 -> 内存 -> 硬盘 -> 压缩 -> 操作系统 -> 可执行文件 -> 应用 -> 汇编语言 -> 硬件 -> BI -> C语言语法

### CPU

- 程序是什么
- 程序由什么组成
- 什么是机器语言
- 运行中的程序存储在哪里
- 什么是内存地址
- 计算机的组件中,哪个负责程序的解释和运行?
- CPU的运行机制

CPU:central processing unit 中央处理器.负责解释和运行程序内容.    
IC=CPU+内存    
CPU=寄存器,控制器,运算器,时钟     
寄存器: 暂存指令,数据等处理对象,内存的一部分.一个CPU有20-100个寄存器    
控制器:把内存指令,数据读入到寄存器,并根据指令的执行结果控制整个计算机    
运算器: 运算从内存读入寄存器的数据    
时钟: 发出CPU开始计时的时钟信号     

内存: 指计算机的主存储器(main memory),简称主存.通过控制芯片等于CPU相连,主要负责存储指令和数据.

程序运行机制: 程序启动后,根据时钟信号,控制器从内存读取指令和数据后,进行解释和运行;运算器对数据进行运算.控制器根据该运行结果来控制计算机.    
控制:数据运算之外的处理,主要是数据输入输出的时机控制.如键盘的输入输出等.

- 寄存器

CPU的核心是寄存器,因为程序把寄存器作为对象来描述.    
汇编: 汇编语言编写的程序转化成机器语言的过程;    
反汇编: 机器语言程序转化成汇编语言的过程

```
mov eax, dword ptr [ebp-8]   把数值从内存复制到eax    
add eax, dword ptr [ebp-0Ch] eax的数据和内存的数值相加    
mov dword ptr [ebp-4], eax   把eax的数值存储到内存
```
上面是汇编语言转成成机器语言的程序执行情况.机器语言级别的程序是通过寄存器来处理的.CPU是寄存器的集合体

程序的处理过程就是将高级语言程序编译后转化为机器语言,再通过CPU内部的寄存器等进行处理.

寄存器可以分为8类.寄存器的存储内容可以是指令也可以是数据.    
数据:运算数值,内存地址的数值    

![寄存器分类](https://upload-images.jianshu.io/upload_images/4933701-4cc3831e6b621953.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

程序计数器,累加寄存器,标志寄存器,指令寄存器和栈寄存器只有一个,其他的一般有多个

- 计数器

cpu计数器存放下一条指令的执行地址.CPU控制器每次读取计数器的地址来获取内存中的数据,完成程序的执行.

1 程序的流程与计数器

程序的流程: 顺序执行,条件执行,循环.    
程序的流程是通过计数器来控制指令执行顺序,通过标志寄存器来决定条件和循环的跳转行为;标志寄存器中存储了当前执行的运算结果;

![条件分支下的计数器](https://img1.baidu.com/it/u=3728839880,2864243253&fm=253&fmt=auto&app=138&f=JPG?w=991&h=500)

2 函数与计数器

函数调用处理通过把程序计数器的值设置为函数的存储地址.但函数的调用需要入口地址和出口地址,单纯的计数器无法在一个计数上标记两个地址.
这时就需要栈首先存入函数执行结束后的地址,函数执行结束后将结果值读取并设定到程序计数器中对应的地址上.函数执行完全在栈中完成即可.

![函数与计数器-栈](https://img0.baidu.com/it/u=3382044954,2575628411&fm=253&fmt=auto&app=138&f=PNG?w=635&h=479)

- 基址寄存器和变址寄存器与数组

通过基址寄存器和变址寄存器将内存上特点的区域划分,实现数组的功能.基址寄存器相当于数组的起始角标,变址寄存器相当于索引,两个地址组合即可获取到对象的实际存储位置.

机器语言指令类型和功能:    

> 数据转送指令：寄存器和内存、内存核内存、寄存器和外围设置之间的数据读写操作    
运算指令：用累加寄存器执行算术运算、逻辑运算、比较运算、移位运算    
跳转指令：实现条件分支、循环、强制跳转等    
call/return指令：函数的调用/返回调用前的地址    


### 二进制

1 32位是几个字节    
2 二进制数 01011100 转换成十进制是多少    
3 二进制数 00001111 左移两位后,变成原来的几倍    
4 补码形式表示的8位二进制数 11111111 , 用十进制表示是多少    
5 补码形式的8位二进制数 10101010,用16位二进制数表示是多少    
6 反转部分图形模式时,使用的是什么逻辑运算
7 为什么用二进制表示数据    
8 -1如何用二进制表示 

计算机内部由集成电路IC构成.IC的引脚只有直流电压0V,5V两种状态.这个特性决定了计算机的信息数据使用二进制表示非常吻合.IC引脚的状态就表示为二进制的0,1,11...    
计算机的最小单位--位(bit:binary digit),相当于二进制中的一位.二进制的位数一般是8的倍数,因为计算机所处理的信息的基本单位是8位二进制数.8位二进制被称为一个字节(byte).    
字节是最基本的信息计量单位.位是最小单位,字节是基本单位.    
字节单位处理数据时,若数据小于存储数据的字节数,高位用0填补.如:1011,用8位表示为00001011,16位表示0000000000001011.当然还有32位,64位....      
二进制数可以表示任意数据,可以是数字,文字,图片,视频等.

- 二进制数

进制数=SUM(数值*位权)
位权=基数^(位数-1)
 
如十进制数39,它的基数为10,几进制它的基数就是几.    
即: 39=3*10^(2-1)+9*10^(1-1)=3*10+9*1

所以二进制1011=1*2^(4-1)+0*2^(3-1)+1*2^(2-1)+1*2^(1-1)=2^3+0+2^1+2^0=11

移位运算相当于乘除运算.左移(向高位移)<<相当于乘法运算,右移(向低位移)>>相当于除法运算.所以溢出位直接被舍弃    
左移空出低位补0;右移空出高位,需要根据右移运算规则判断.

二进制中使用加法来进行减法运算.
负数: 二进制的补数.核心法则:正数+负数=0    
补数: 正数来表示负数.二进制数取反+1.    
所以00000001表示1,-1的二进制数为: 1取反->11111110+1=11111111,而不是直观上的10000001.    
因为:1+(-1)=0而 00000001 +10000001=10000010 = -126 != 0 
正确的是: 1+(-1)=00000001 + 11111111 = 100000000 溢出首位舍弃则为 00000000 

由于所有的减法是通过加法实现.所以3-5=3+(-5)
即:     
3= 00000011
5= 00000101 5取反+1 -> 11111010 + 00000001 = 11111011
最终等于: 00000011+11111011=11111110=-2

-2的补数为-2取反+1=00000001+1=00000010

同样的位数包含符号位与不包含符号位数据量相同,但是取值范围不同.如short(float)类型能表示2^16=65536个值.其中unsigned short取值范围为0-65535,short类型取值范围为:-32768-32767.因为最高为1的表示负数,所以0归到了正数中.整个数值总数为2的偶数倍.

- 右移运算规则    
逻辑右移:高位填0.如 11111100,逻辑右移两位为: 00111111=63    
算术右移:高位不变.负数高位填1,正数高位填0.如:11111100,算术运算两位为: 11111111=-1

> 问题: 11111100算术右移5位是多少?计算器中无法计算算术右移后小于-1的值

符号扩充: 正数高位补0,负数高位补1.8位二进制数 00001111 扩充为16位二进制数为: 00000000 00001111 

- 逻辑运算

将二进制数作四则运算的数值为算术.像图形模式将数值作0,1的罗列为逻辑.    
算术运算:加减乘除    
逻辑运算:逻辑非NOT,逻辑与AND,逻辑或OR,逻辑异或XOR

逻辑非:0变1,1变0的取反操作;    
逻辑与:两个值都是1则为1,否则为0    
逻辑或:只要有一个1即为1    
逻辑异或: 排斥相同数值的运算.一方为1,一方为0,结果为1,否则都是0

二进制数:0为假(false),1为真(true)


### 小数运算

1 二进制数0.1,用十进制表示的话是多少    
2 用小数点后有三位的二进制数,能表示十进制数0.625吗    
3 将小数分为符号,尾数,基数,指数4部分进行表现的形式称为什么    
4 二进制数的基数是多少    
5 把0作为数值范围的中间值,而不是用符号位的情况来表示负数的表示方法称为什么    
6 10101100.01010011用十六进制数表示是多少    
7 移位是否会造成精度损失?会,只要涉及小数进制转换就可能存在精度损失.位移也需要将10进制转为2进制进行计算

### 小数

二进制小数和整数的计算规则一样都等于: 基数*位权

如:二进制转十进制 1011.0011 = 1*2^3+0*2^2+1*2^1+1*2^0+0*2^-1+0*2^-2+1*2^-3+1*2^-4=1*8+0*4+1*2+1*1+0*0.5+0*0.25+1*0.125+1*0.0625=11.1875


![二进制四位小数转十进制](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic3.zhimg.com%2Fv2-d5313d5427f04a17974b040992310c9a_b.jpg&refer=http%3A%2F%2Fpic3.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1663462882&t=309ba6b783cae0b65645ed5966ee506b)

- 计算机运算错误原因: 二进制的小数无法和十进制完全建立映射关系    
由于二进制和十进制由于循环小数以及精度的限制,无法完全相等,计算机只能通过截断或四舍五入,这样就造成了计算机运算错误.    
如十进制0.1,二进制表示为0.0100110011001100110011001100110011001100110011001101.

- 浮点数

浮点数: 像0.12345*10^3和0.12345*10^-1这样使用与实际小数点位置不同的书写方法来表示小数的形式.与其相对的是定点数.    
定点数: 小数的小数点的实际位置固定不变.如:123.45,0.012345.

程序中使用双精度浮点数,单精度浮点数来表示小数.双精度浮点数类型使用64位,单精度浮点数使用32位表示所有小数.

浮点数使用符号,尾数,基数,指数四部分表示小数.因为计算机使用二进制表示数据,所以基数为2.所以符号,尾数,指数三部分可以表示浮点数       

![浮点数公式](http://image.bubuko.com/info/201707/20180110235911806090.png)    
![浮点数](https://pic3.zhimg.com/80/v2-603331d7d99b75a7b5dea8fe96259172_720w.jpg)


- 符号:0为正,1为负.数值大小用尾数和指数表示    
- 尾数部分:使用正则表达式表示.正则表达式:按照特定的规则来表示数据的形式.为了统一数据表示形式,避免表示形式太多    
如:
> 十进制浮点数遵循: 小数点前面是0,小数点后面第一位不能是0;0.75=0.75*10^0    
> 二进制浮点数遵循: 小数点前面是1.小数要进行逻辑移位.    

![单精度浮点数表示](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimage.bubuko.com%2Finfo%2F201707%2F20180110235911810973.png&refer=http%3A%2F%2Fimage.bubuko.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1663554115&t=7eaea8dde17bbf74074da078c82e3f0a)

浮点数中移位后第一位会被省略,实际浮点数可以表示24的数值.小数点后23位.

- 指数部分: 使用EXCESS系统表现.    

EXCESS系统表现指在不使用符号位情况下表示负数.它通过将指数部分表示范围的中间值设为0,使得负数不需要用符号来表示.    
如:当指数部分是8位单精度浮点数时,最大值为 11111111(255),它的中间值0为 255/2=127->01111111,00000000表示为-127.    
其实数值的具体含义可以人为控制,只要能自圆其说即可.所有的数据本身就是一种计算,计量规则,它本身的表示形式是多样的.

- 小数十进制转二进制

十进制0.75的单精度浮点数表示为: 0-01111110-10000000000000000000000.含义分析    
0:符号位,它为正数    
01111110: 指数部分,值为126,由于0为127,所以它的EXCESS表现为126-127=-1         
10000000000000000000000: 它的浮点数形式为: 1.10000000000000000000000=1*2^0+1*2^-1=1.5    
即: 0-01111110-10000000000000000000000=+1.5^-1=0.75    

- 二进制与十六进制

十六进制是4位二进制表示一个数的表示方式.为了缩短二进制数的位数.表示方式为: 0x...    
32位二进制数使用16进制表示只需要8位即可.转换时每四位等于16进制值的一位,不足使用0补位.    
如: 1011.011->1011.0110=B.6

#### 避免精度损失方法

1. 回避策略.可以接受误差,进行四舍五入    
2. 把小数转为整数计算.先把小数转为整数,计算结果再转为小数.整数计算不涉及精度损失    
3. BCD(Binary Coded Decimal).二进制表示十进制的方法.使用4位二进制表示0-9的数字的处理方法.它不会有精度损失,常在财务中应用.


### 内存

> 1 有十个地址信号引脚的内存IC(集成电路)可以指定的地址范围是多少    
2 高级编程语言中的数据类型表示的是什么    
3 在32位内存地址的环境中,指针变量的长度是多少位    
4 与物理内存有着相同构造的数组的数据类型长度是多少        
5 用LIFO方式进行数据读写的数据结构称为什么    
6 根据数据的大小链表分叉成两个方向的数据结构称为什么    

























